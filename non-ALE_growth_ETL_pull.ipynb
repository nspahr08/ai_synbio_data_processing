{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0e6234-ca02-4344-a5d8-3ffc0d4cf0d2",
   "metadata": {},
   "source": [
    "# ETL and Pull OD data for growth experiments that are not generated by robotic ALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the data submission web UI, there will need to be a way to \n",
    "# 1) Register a protocol\n",
    "# 2) Register strains and growth condtions,\n",
    "# 3) Register an experiment\n",
    "# 4) Upload data/metadata formatted according to predefined specifications. \n",
    "\n",
    "# Code below assumed that 1,2,3, are already in the database.\n",
    "# 4 will be pulled from the MinIO storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c650e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import minio\n",
    "import io\n",
    "from amiga.libs.growth import GrowthPlate\n",
    "from amiga.libs.model import GrowthModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e519406f-8733-4823-ba0e-b02b6cc6dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELETING EXPERIMENT AND OPERATION#####\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "engine = create_engine(\n",
    "    (\n",
    "        \"mysql+pymysql://nspahr:henrylab@poplar.cels.anl.gov/\"\n",
    "        \"anl_synbio?charset=utf8mb4\"\n",
    "    )\n",
    ")\n",
    "\n",
    "statements = [\n",
    "    # \"\"\"DELETE FROM `experiment` WHERE `id`='ALE1b';\"\"\", \n",
    "    \"\"\"DELETE FROM `operation` WHERE `id`='csr_crc_exp_operation';\"\"\"\n",
    "]\n",
    "\n",
    "for s in statements:\n",
    "    \n",
    "    with engine.connect() as con:\n",
    "        con.execute(text(s))\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78753c77-f656-4e8a-b8c4-e3d7e0155216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGISTER EXPERIMENT WITH DB\n",
    "\n",
    "engine = create_engine(\n",
    "    (\n",
    "        \"mysql+pymysql://nspahr:henrylab@poplar.cels.anl.gov/\"\n",
    "        \"anl_synbio?charset=utf8mb4\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def register_operation(op_id, protocol_id, lab_id, contact_id, timestamp):\n",
    "    operation_dict = {\n",
    "        'id': [op_id],\n",
    "        'protocol_id': [protocol_id],\n",
    "        'lab_id': [lab_id],\n",
    "        'contact_id': [contact_id],\n",
    "        'timestamp': [timestamp]\n",
    "    }\n",
    "    operation_df = pd.DataFrame.from_dict(operation_dict)\n",
    "    operation_df.to_sql('operation', engine, index=False, if_exists='append')\n",
    "\n",
    "\n",
    "def register_experiment(experiment_id, exp_type, start_date, exp_index, description, op_id):\n",
    "    exp_dict = {\n",
    "        'id': [experiment_id],\n",
    "        'type': [exp_type],\n",
    "        'start_date': [start_date],\n",
    "        'index': [exp_index],\n",
    "        'description': [description],\n",
    "        'operation_id': [op_id]\n",
    "    }\n",
    "    new_exp_df = pd.DataFrame.from_dict(exp_dict)\n",
    "    new_exp_df.to_sql('experiment', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a60a9ae-1223-45a4-bf7b-78778759f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operation and experiment\n",
    "operation = 'csr_crc_exp_operation'\n",
    "experiment = 'csr_crc_UGA_2025-05-05'\n",
    "\n",
    "# Folder name in minio that has to contain csv with metadata(metadata.csv) and data (hour_OD.csv)\n",
    "folder_name = 'csr_crc_UGA_2025-05-05/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea62745-8603-4a66-acdc-cdfe5b6f1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_operation('csr_crc_exp_operation', 'mock_growth_kinetics_protocol', 2, 2, \"2025-05-05\")\n",
    "register_experiment('csr_crc_UGA_2025-05-05', 'growth', \"2025-05-05\", 1, '', 'csr_crc_exp_operation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad53fd7-f753-4cf5-a4cf-9bd34a8f33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_exp_ETL(procedure, experiment, minio_folder):\n",
    "    # DB Connections\n",
    "    \n",
    "    engine = create_engine(\n",
    "        (\n",
    "            \"mysql+pymysql://nspahr:henrylab@poplar.cels.anl.gov/\"\n",
    "            \"anl_synbio?charset=utf8mb4\"\n",
    "        )\n",
    "    )\n",
    "    mio = minio.Minio(\n",
    "        'poplar.cels.anl.gov:9000',\n",
    "        secret_key=\"henry-minion\",\n",
    "        access_key=\"henrylab\",\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    minio_bucket_name = 'synbio'\n",
    "    path_to_data = folder_name + 'hour_OD.csv'\n",
    "    path_to_metadata = folder_name + 'metadata.csv'\n",
    "\n",
    "    # Read metadata\n",
    "    response = mio.get_object(\n",
    "        minio_bucket_name, path_to_metadata\n",
    "        )\n",
    "    csv_data = response.data\n",
    "    mapping = pd.read_csv(io.BytesIO(csv_data))\n",
    "    \n",
    "    # Read data\n",
    "    response = mio.get_object(\n",
    "        minio_bucket_name, path_to_data\n",
    "        )\n",
    "    csv_data = response.data\n",
    "    raw = pd.read_csv(io.BytesIO(csv_data))\n",
    "    \n",
    "    data_cols = ['time'] + mapping['sample_number'].to_list()\n",
    "    \n",
    "    data = raw.copy()\n",
    "    data.columns = data_cols\n",
    "    \n",
    "    # Prepare data tables for db upload\n",
    "    mapping['experiment_id'] = experiment\n",
    "    mapping['operation_id'] = operation\n",
    "    mapping['measurement_type'] = 'growth'\n",
    "    mapping['filename'] = path_to_data\n",
    "    mapping['sample_name'] = ('E:' + experiment + \n",
    "                              '.S:' + mapping['strain_id'].astype(str) + \n",
    "                              '.C:' +  mapping['growth_condition_id'].astype(str) + \n",
    "                              '.R:' + mapping['replicate'].astype(str))\n",
    "    \n",
    "    samples = mapping[['sample_name', 'experiment_id', 'growth_condition_id', 'strain_id', 'replicate']]\n",
    "    samples.rename(columns={'sample_name':'name'}, inplace=True)\n",
    "    samples.to_sql('sample', engine, index=False, if_exists='append')\n",
    "    \n",
    "    measurements = mapping[['sample_name', 'operation_id', 'measurement_type', 'filename']]\n",
    "    measurements.rename(columns={'sample_name':'sample_id', 'measurement_type':'type'}, inplace=True)\n",
    "    measurements.to_sql('measurement', engine, index=False, if_exists='append')\n",
    "    \n",
    "    od = pd.melt(data, id_vars=['time'], var_name='sample_number', value_name='od')\n",
    "    \n",
    "    sample_names = tuple(samples['name'])\n",
    "    \n",
    "    meas_from_db = pd.read_sql(\n",
    "        f\"SELECT `id`, `sample_id` FROM `measurement` WHERE `sample_id` IN {sample_names};\",\n",
    "        engine\n",
    "        ).rename(columns={'id': 'measurement_id'})\n",
    "    \n",
    "    od_meas = od.merge(mapping[['sample_name', 'sample_number']], on='sample_number', how='inner'\n",
    "                        ).merge(meas_from_db, left_on='sample_name', right_on='sample_id', how='inner'\n",
    "                               ).drop(['sample_name','sample_id', 'sample_number'], axis=1\n",
    "                                     ).rename(columns={'time':'timepoint'})\n",
    "    \n",
    "    od_meas.to_sql('od_measurement', engine, index=False, if_exists='append')\n",
    "    \n",
    "    \n",
    "    mapping.set_index('sample_number', inplace=True)\n",
    "    \n",
    "    plate = GrowthPlate(data=data,key=mapping)\n",
    "    plate.computeBasicSummary()\n",
    "    plate.raiseData()\n",
    "    plate.logData()\n",
    "    plate.subtractBaseline(True,poly=False) \n",
    "    \n",
    "    metrics = plate.key[['sample_name','OD_Max']]\n",
    "    \n",
    "    for sample in plate.data.columns:\n",
    "        # fit curve\n",
    "        thisSample = pd.concat([plate.time, plate.data[sample]], axis=1).dropna() #### There were some missing values, so had to drop those, otherwise curve fitting won't work\n",
    "        thisSample.columns = ['Time', 'OD']\n",
    "        gm = GrowthModel(thisSample,ARD=True)\n",
    "        gm.fit()\n",
    "        curve = gm.run()\n",
    "        metrics.loc[sample, 'growth_rate'] = curve.gr\n",
    "        metrics.loc[sample, 'doubling_time'] = curve.td * 60\n",
    "        metrics.loc[sample, 'lag_time'] = curve.lagC * 60\n",
    "    \n",
    "    # Massage data so it will fit into database\n",
    "    # i.e., replace infinity values, replace very large values with NA,\n",
    "    # and round to 3 dec places, \n",
    "    metrics = metrics.merge(meas_from_db, left_on='sample_name',right_on='sample_id', how='inner'\n",
    "                           ).rename(columns={'OD_Max': 'max_od'}\n",
    "                                   ).drop(['sample_name', 'sample_id'], axis=1)\n",
    "    \n",
    "    metrics['lag_time'] = metrics['lag_time'].replace([np.inf, -np.inf], 10000)\n",
    "    metrics['doubling_time'] = metrics['doubling_time'].replace([np.inf, -np.inf], 10000)\n",
    "    metrics['max_od'] = metrics['max_od'].apply(lambda x: round(x, 3))\n",
    "    metrics['growth_rate'] = metrics['growth_rate'].apply(lambda x: round(x, 3))\n",
    "    \n",
    "    crazy_lag_time = (metrics['lag_time'] >= 10000) | (metrics['lag_time'] < 0)\n",
    "    metrics['lag_time'] = np.where(\n",
    "        crazy_lag_time,\n",
    "        [pd.NA]*len(metrics),\n",
    "        metrics['lag_time'].apply(round)\n",
    "    )\n",
    "    \n",
    "    crazy_doubling_time = (metrics['doubling_time'] >= 10000) | (metrics['doubling_time'] < 0)\n",
    "    metrics['doubling_time'] = np.where(\n",
    "        crazy_doubling_time,\n",
    "        [pd.NA]*len(metrics),\n",
    "        metrics['doubling_time'].apply(round)\n",
    "    )\n",
    "    \n",
    "    od_max_near_0 = metrics['max_od'] < 0.0001\n",
    "    metrics['max_od'] = np.where(\n",
    "        od_max_near_0,\n",
    "        [pd.NA]*len(metrics),\n",
    "        metrics['max_od']\n",
    "    )\n",
    "    growth_rate_near_0 = metrics['growth_rate'] < 0.0001\n",
    "    metrics['growth_rate'] = np.where(\n",
    "        growth_rate_near_0,\n",
    "        [pd.NA]*len(metrics),\n",
    "        metrics['growth_rate']\n",
    "    )\n",
    "    \n",
    "    metrics.to_sql('growth_measurement', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d1b5777-4b74-409a-bc72-edce63026b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /tmp/ipykernel_123612/3120152782.py:51: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " /tmp/ipykernel_123612/3120152782.py:55: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " /tmp/ipykernel_123612/3120152782.py:92: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " /tmp/ipykernel_123612/3120152782.py:93: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " /tmp/ipykernel_123612/3120152782.py:94: SettingWithCopyWarning:\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "growth_exp_ETL(operation, experiment, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd4fce-70b6-4a4d-80b3-224f6657ec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d9348-8db0-4162-b0b9-7292c1fc1c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d6245-269d-4b3f-a97a-7443f157eddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "330818a1-c397-4930-a8cf-cf3adc57cd6c",
   "metadata": {},
   "source": [
    "## Pull data (and plot)\n",
    "\n",
    "Given an experiment id and a strain id, pull the corresponding samples form the database and present in pandas DataFrame.\n",
    "The function plot_OD() plots the data (using matplotlib).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f97f7-c68e-490c-8930-be92994bdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def query_OD(experiment_id, strain_id):\n",
    "    '''\n",
    "    Queries db for all samples from specified experiment and returns all \n",
    "    associated od_measurements as pandas DataFrame. Plots all samples of\n",
    "    specified strain_id.\n",
    "\n",
    "    Args:\n",
    "        experiment_id (str): Experiment id. Must be in db.\n",
    "        strain_id (str): Strain id. Must be in db.\n",
    "    Returns:\n",
    "        DataFrame    \n",
    "    '''\n",
    "    \n",
    "    # DB Connection\n",
    "    engine = create_engine(\n",
    "        (\n",
    "            \"mysql+pymysql://nspahr:henrylab@poplar.cels.anl.gov/\"\n",
    "            \"anl_synbio?charset=utf8mb4\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Hardcode experiment id. To be changed later\n",
    "    experiment_id = 'ALE1b'\n",
    "\n",
    "    # Check validity of passed arguments\n",
    "    db_experiments = pd.read_sql(\n",
    "        \"SELECT experiment.id FROM experiment\", engine\n",
    "    )['id'].to_list()\n",
    "\n",
    "    db_strains = pd.read_sql(\n",
    "        \"SELECT strain.id FROM strain\", engine\n",
    "    )['id'].to_list()\n",
    "\n",
    "    if (\n",
    "        experiment_id not in db_experiments\n",
    "    ) or (\n",
    "        strain_id not in db_strains\n",
    "    ):\n",
    "        print(f\"Check if experiment and strain are registered in the db.\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    # Hardcoded query. This can be made more flexible later.\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        experiment.id,\n",
    "        sample.name, sample.passage, \n",
    "        sample.strain_id, strain.long_name,\n",
    "        sample.growth_condition_id, growth_condition.carbon_source,\n",
    "        measurement.type,\n",
    "        od_measurement.datetime, od_measurement.od, od_measurement.background\n",
    "    FROM \n",
    "        experiment\n",
    "        INNER JOIN sample ON sample.experiment_id = experiment.id\n",
    "        INNER JOIN measurement ON measurement.sample_id = sample.name\n",
    "        INNER JOIN od_measurement ON od_measurement.measurement_id = measurement.id\n",
    "        INNER JOIN strain ON strain.id = sample.strain_id\n",
    "        INNER JOIN growth_condition ON growth_condition.id = sample.growth_condition_id\n",
    "        \n",
    "    WHERE \n",
    "        (experiment.id=%(experiment)s) AND (sample.strain_id=%(strain)s)\n",
    "    \"\"\"\n",
    "    \n",
    "    selection = pd.read_sql(\n",
    "        query, engine, params={'experiment': experiment_id, 'strain': str(strain_id)}\n",
    "    ).rename(\n",
    "        columns={'id': 'experiment_id',\n",
    "                 'name': 'sample_name',\n",
    "                 'type': 'measurement_type',\n",
    "                 'long_name':'strain_name'}\n",
    "    )\n",
    "\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fc85e-efcd-4649-bbea-a67162802e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = query_OD('not_a_real_argument', 2)\n",
    "selection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2e17b-29ea-4c80-8926-a4326165a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from statistics import mean, median\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_OD(df, subtract_background = False, yscale='log', append_title=''):\n",
    "    '''\n",
    "    Plots OD measurements from DataFrame that is returned from od_query function.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe returned from od_query() function.\n",
    "        subtract_background (bool): Whether to subtract background reading \n",
    "            from all measurements.\n",
    "        yscale (str): 'log' or 'linear'\n",
    "        append_title (str): Additional text to add to the figure title\n",
    "    Returns:\n",
    "        None    \n",
    "    '''\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    df = df.sort_values('datetime')\n",
    "    df['od_background_subtracted'] = df['od'] - df['background']\n",
    "    \n",
    "    if subtract_background:\n",
    "        value = 'od_background_subtracted'\n",
    "    else:\n",
    "        value = 'od'\n",
    "\n",
    "    # Define different combinations of conditions that will be plotted\n",
    "    conditions = df[['carbon_source', 'strain_name']].drop_duplicates().dropna()\n",
    "    conditions['label'] = conditions.apply(\n",
    "        lambda x: f'{x[\"strain_name\"]} - {x[\"carbon_source\"]}', axis=1\n",
    "    )\n",
    "    conditions['colors'] = colormaps['tab20'].colors[:len(conditions)]\n",
    "\n",
    "    # For the legend\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Figure will need to be stretched horizontally for readability\n",
    "    fig_width, fig_height = fig.get_size_inches() # Get the current figure size\n",
    "    fig.set_size_inches(fig_width * 3.5, fig_height) # Doubling the width\n",
    "    \n",
    "    total_transfers = df['passage'].max()\n",
    "\n",
    "    # For the defined conditions and at each transfer,\n",
    "    # plot the OD readings\n",
    "    for i, row in conditions.iterrows():\n",
    "        handle_line = Line2D([0], [0], label=row['label'], color=row['colors'])\n",
    "        handles.append(handle_line)\n",
    "        label = row['label']\n",
    "        labels.append(label)\n",
    "        \n",
    "        for t in range(total_transfers+1):\n",
    "            this_condition = df.loc[\n",
    "                (df['carbon_source'] == row['carbon_source']) &\n",
    "                (df['strain_name'] == row['strain_name']) &\n",
    "                (df['passage'] == t)\n",
    "            ]\n",
    "            this_condition = this_condition.groupby('datetime'\n",
    "                                                   )[value].agg(mean).to_frame().reset_index()\n",
    "            plt.plot(\n",
    "                this_condition['datetime'],\n",
    "                this_condition[value],\n",
    "                color=row['colors'],\n",
    "                marker='o',\n",
    "                markersize=2\n",
    "            )\n",
    "            \n",
    "    # Configure and label the axes and tickmarks\n",
    "    plt.yscale(yscale)\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "    plt.xlabel('datetime')\n",
    "    plt.ylabel('OD') \n",
    "\n",
    "    early_datetimes = df.groupby('passage')['datetime'].agg(\n",
    "        lambda x: sorted(list(set(x)))[3])\n",
    "    secax = ax.secondary_xaxis('top')\n",
    "    secax.set_xticks(early_datetimes, np.arange(1, total_transfers+1, 1))\n",
    "    secax.set_xlabel('transfer')\n",
    "\n",
    "    # Set title and legend\n",
    "    plt.title(value + append_title, fontsize=16)\n",
    "    plt.legend(handles=handles, labels=labels, loc='lower center')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78e7cc-e5f6-4e2e-982a-924ef6e644da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_OD(selection, subtract_background = False, yscale='log', append_title='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelSEED",
   "language": "python",
   "name": "modelseed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
