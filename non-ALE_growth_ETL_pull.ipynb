{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0e6234-ca02-4344-a5d8-3ffc0d4cf0d2",
   "metadata": {},
   "source": [
    "# ETL and Pull OD data for growth experiments that are not generated by robotic ALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2dee28-3dc5-484e-95e7-e89ca3a4706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import minio\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def etl():\n",
    "    '''\n",
    "    Extracts and transforms plate reader data from MinIO storage,\n",
    "    and stores it in MySQL database.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "    # DB Connections\n",
    "    \n",
    "    \n",
    "    engine = create_engine(\n",
    "        (\n",
    "            \"mysql+pymysql://nspahr:henrylab@poplar.cels.anl.gov/\"\n",
    "            \"anl_synbio?charset=utf8mb4\"\n",
    "        )\n",
    "    )\n",
    "    mio = minio.Minio(\n",
    "        'poplar.cels.anl.gov:9000',\n",
    "        secret_key=\"henry-minion\",\n",
    "        access_key=\"henrylab\",\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Functions\n",
    "    \n",
    "    \n",
    "    def get_plate_reader_filenames(\n",
    "        minio_bucket, minio_path_to_plate_reader_files, regexpattern\n",
    "    ):\n",
    "        # Return list of file names that match the expected file name pattern.\n",
    "        objects = mio.list_objects(\n",
    "            minio_bucket, prefix=minio_path_to_plate_reader_files\n",
    "        )\n",
    "        filepaths = [obj.object_name for obj in objects]\n",
    "        filenames = [fn.split('/')[1] for fn in filepaths]\n",
    "        plate_reader_files = [fn for fn in filenames if re.match(regexpattern, fn)]\n",
    "        \n",
    "        return plate_reader_files\n",
    "    \n",
    "    \n",
    "    def load_layout(minio_bucket_name, layout_file_path):\n",
    "        # Read plate layout file.\n",
    "        try:\n",
    "            response = mio.get_object(minio_bucket_name, layout_file_path)\n",
    "            csv_data = response.data\n",
    "            df = pd.read_csv(io.BytesIO(csv_data), header=None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            df = None\n",
    "        finally:\n",
    "            response.close()\n",
    "            response.release_conn()\n",
    "            return df\n",
    "    \n",
    "    \n",
    "    def get_transfers(batch_dict, batch, plate, transfer):\n",
    "        # Which transfer since beginning of experiment?\n",
    "        return batch_dict[batch] + (plate-1)*3 + (transfer)\n",
    "    \n",
    "    \n",
    "    def get_plates(batch_dict, batch, plate):\n",
    "        # How many plates in total?\n",
    "        return batch_dict[batch] + plate\n",
    "    \n",
    "    \n",
    "    def get_parent_plate(plate, column):\n",
    "        # Which plate was the parent sample on?\n",
    "        if column in (1, 4, 7):\n",
    "            parent_plate = plate-1\n",
    "        else:\n",
    "            parent_plate = plate\n",
    "        return parent_plate\n",
    "    \n",
    "    \n",
    "    def get_parent_col(col):\n",
    "        # Which column was the parent sample in?\n",
    "        cols = (1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 10, 11)\n",
    "        parent_cols = (3, 1, 2, 6, 4, 5, 9, 7, 8, 0, 0, 0)\n",
    "        parent_col_dict = dict(zip(cols, parent_cols))\n",
    "        return parent_col_dict[col]\n",
    "    \n",
    "    \n",
    "    def get_well_name(row, col):\n",
    "        # Return well name based on plate rows/cols.\n",
    "        well_name = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'][row] + str(col+1)\n",
    "        return well_name\n",
    "    \n",
    "    \n",
    "    # Hardcoded variables. \n",
    "    # Later, these things should be supplied when the experimental team \n",
    "    # registers the experiment through the web UI.\n",
    "    minio_bucket_name = 'synbio'\n",
    "    path_to_plate_reader_files = 'ALE1b_OD_data/'\n",
    "    path_to_layout_files = 'plate_layouts/'\n",
    "    fname_pattern = re.compile(\n",
    "        r'(?P<experiment>\\w+)_(?P<timestamp>\\d+)_(?P<uniqueID>\\w+)_(?P<plate>\\d+)_(?P<transfer>[1-3])_(?P<timepoint>\\d+).txt'\n",
    "        )\n",
    "    experiment_id = path_to_plate_reader_files.split('_')[0]\n",
    "    plate_type = '96_shallow' # could also be deep well plate\n",
    "    start_date = '2025-04-04'\n",
    "    exp_index = 1\n",
    "    exp_type = 'autoALE'\n",
    "    description = ''\n",
    "    protocol_id = 'mock_1b_protocol'\n",
    "    lab_id = 1\n",
    "    contact_id = 1\n",
    "    operation_id = f\"{experiment_id}_operation\"\n",
    "    measurement_type = 'growth'\n",
    "    plate_reader_filenames = get_plate_reader_filenames(\n",
    "        minio_bucket_name, path_to_plate_reader_files, fname_pattern\n",
    "        )\n",
    "    transfer_layout = load_layout(\n",
    "        minio_bucket_name, path_to_layout_files + 'transfer_layout.csv'\n",
    "        )\n",
    "    rep_layout = load_layout(\n",
    "        minio_bucket_name, path_to_layout_files + 'replicate_layout.csv'\n",
    "        )\n",
    "    strain_layout = load_layout(\n",
    "        minio_bucket_name, path_to_layout_files + 'strain_layout.csv'\n",
    "        )\n",
    "    gc_layout = load_layout(\n",
    "        minio_bucket_name, path_to_layout_files + 'growth_condition_layout.csv'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Upload this experiment and its operation (i.e., procedure) to the db.\n",
    "    operation_dict = {\n",
    "        'id': [operation_id],\n",
    "        'protocol_id': [protocol_id],\n",
    "        'lab_id': [lab_id],\n",
    "        'contact_id': [contact_id],\n",
    "        'timestamp': [start_date]\n",
    "    }\n",
    "    exp_operation_df = pd.DataFrame.from_dict(operation_dict)\n",
    "    exp_operation_df.to_sql('operation', engine, index=False, if_exists='append')\n",
    "    \n",
    "    exp_dict = {\n",
    "        'id': [experiment_id],\n",
    "        'type': [exp_type],\n",
    "        'start_date': [start_date],\n",
    "        'index': [exp_index],\n",
    "        'description': [description],\n",
    "        'operation_id': [operation_id]\n",
    "    }\n",
    "    new_exp_df = pd.DataFrame.from_dict(exp_dict)\n",
    "    new_exp_df.to_sql('experiment', engine, index=False, if_exists='append')\n",
    "    \n",
    "    # Initialize data df\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # Read info from plate reader file names and file content into df\n",
    "    for f in plate_reader_filenames:\n",
    "    \n",
    "        try:\n",
    "    \n",
    "            # Initialize row in dataframe\n",
    "            data_row = {}\n",
    "            match = fname_pattern.match(f)\n",
    "    \n",
    "            # Parse info contained in plate reader file name\n",
    "            data_row['experiment'] = str(match.group('experiment'))\n",
    "            data_row['file_ID'] = str(match.group('uniqueID'))\n",
    "            data_row['timestamp'] = int(match.group('timestamp'))\n",
    "            data_row['plate_index'] = int(match.group('plate'))\n",
    "            # t_transfer indicates which plate cols were most recently innoculated.\n",
    "            data_row['t_transfer'] = int(match.group('transfer'))\n",
    "    \n",
    "            # Read plate reader files into dataframe\n",
    "            response = mio.get_object(\n",
    "                minio_bucket_name, path_to_plate_reader_files+f\n",
    "                )\n",
    "            csv_data = response.data\n",
    "            plate_data = pd.read_csv(io.BytesIO(csv_data), header=None)\n",
    "            for row in range(8):\n",
    "                for col in range(12):\n",
    "                    data_row['row'] = row\n",
    "                    data_row['column'] = col\n",
    "                    data_row['OD'] = plate_data.iloc[row, col]\n",
    "                    data = pd.concat([data, pd.Series(data_row).to_frame().T])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "        finally: \n",
    "            response.close()\n",
    "            response.release_conn()\n",
    "    \n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Translate row and column numbers to well names\n",
    "    data['well'] = data.apply(\n",
    "        lambda x: get_well_name(x['row'], x['column']), axis=1\n",
    "        )\n",
    "    # Translate timestamp into isoformat time\n",
    "    data['datetime'] = data['timestamp'].apply(\n",
    "        lambda x: datetime.fromtimestamp(x).isoformat()\n",
    "        )\n",
    "    \n",
    "    # Appending metadata\n",
    "    data['filename'] = path_to_plate_reader_files\n",
    "    data['measurement_type'] = measurement_type\n",
    "    data['experiment'] = experiment_id\n",
    "    data['plate_type'] = plate_type\n",
    "    data['start_date'] = start_date\n",
    "    data['exp_index'] = exp_index\n",
    "    data['operation_id'] = operation_id\n",
    "    data['layout_filename'] = path_to_layout_files \n",
    "    # Given location on plate (row, col) and layout files, get the strain, \n",
    "    # growth condition, replicate number, and transfer_l for each well.\n",
    "    # (transfer_l indicates transfer based on plate location.)\n",
    "    data['strain'] = data.apply(\n",
    "        lambda x: strain_layout.iloc[x['row'], x['column']], axis=1\n",
    "        ).astype(\"Int64\")\n",
    "    data['replicate'] = data.apply(\n",
    "        lambda x: rep_layout.iloc[x['row'], x['column']], axis=1\n",
    "        ).astype(\"Int64\")\n",
    "    data['gc'] = data.apply(\n",
    "        lambda x: gc_layout.iloc[x['row'], x['column']], axis=1\n",
    "        ).astype(\"Int64\")\n",
    "    data['l_transfer'] = data.apply(\n",
    "        lambda x: transfer_layout.iloc[x['row'], x['column']], axis=1\n",
    "        ).astype(\"Int64\")\n",
    "    \n",
    "    # Determine innoculation status\n",
    "    # Explanation: All wells on a plate are read at each timepoint, but only some\n",
    "    # of the wells will be inoculated at any given timepoint. Only wells that have\n",
    "    # media in them (i.e., growth condition not NA) are considered *samples*.\n",
    "    # Samples that do not have a strain designation are negative controls.\n",
    "    # Depending on the location on the plate, samples can have 11, 22, or 33\n",
    "    # timepoints. The calculation below determines which transfer to assign a\n",
    "    # given OD reading to.\n",
    "    \n",
    "    # Calculate actual transfer number for a given reading\n",
    "    data['l-t_transfer'] = data.apply(\n",
    "        lambda x: x['l_transfer']-x['t_transfer'], axis=1\n",
    "        )\n",
    "    data['transfer'] = np.where(\n",
    "        (data['l-t_transfer']<=0), data['l_transfer'], 1\n",
    "        )\n",
    "    \n",
    "    # Compute total number of plates, transfers for this experiment\n",
    "    # There are 3 transfers per plate, x plates per batch, and y batches per\n",
    "    # experiment. A batch is defined as a continuous run of the robot without\n",
    "    # any interruption. All measurements in a batch will have the same file_ID.\n",
    "    # Each batch starts with plate=1, transfer=1, timepoint=1. To accurately\n",
    "    # calculate the cummulative number of transfers since the beginning of the\n",
    "    # experiment for each data point, we need to calculate the number of transfers\n",
    "    # for the individual batches and their cumulative number (in their correct \n",
    "    # order).\n",
    "    \n",
    "    batches = (\n",
    "        data.groupby('file_ID')['datetime']\n",
    "        .agg(\"min\")\n",
    "        .reset_index()\n",
    "        .sort_values('datetime')['file_ID']\n",
    "        .to_list()\n",
    "    )\n",
    "    batch_n_transfers = [0]  # transfers for each of the batches.\n",
    "    # Start with 0, because the first batch has no prior transfers.\n",
    "    batch_n_plates = [0]  # number of plates for each of the batches.\n",
    "    # Start with 0, because first batch has no prior plates.\n",
    "    \n",
    "    for batch in batches:\n",
    "        # Loop through each batch and calculate the number of transfers\n",
    "        batch_data = data.loc[\n",
    "            (data['file_ID'] == batch) & (~pd.isna(data['transfer']))\n",
    "        ]\n",
    "        # From last measurement in batch, total number of plates and transfers in\n",
    "        # this batch\n",
    "        max_plate = batch_data.sort_values(['plate_index', 'transfer']).iloc[-1]\n",
    "        batch_n_plates.append(max_plate['plate_index'])\n",
    "        batch_transfers = (max_plate['plate_index']-1)*3 + (max_plate['transfer'])\n",
    "        batch_n_transfers.append(batch_transfers)\n",
    "    \n",
    "    # cumulative transfers (# of transfers since the start of the experiment)\n",
    "    batch_cumsum_t = np.cumsum(np.array(batch_n_transfers[:-1])) \n",
    "    batch_dict_t = dict(zip(batches, batch_cumsum_t))\n",
    "    # cumulative plates (# of plates since the start of the experiment)\n",
    "    batch_cumsum_p = np.cumsum(np.array(batch_n_plates[:-1])) \n",
    "    batch_dict_p = dict(zip(batches, batch_cumsum_p))\n",
    "    \n",
    "    data['cum_transfer'] = np.where(\n",
    "        data['transfer']==0, np.array([0]*len(data)), \n",
    "        data.apply(\n",
    "            lambda x: get_transfers(\n",
    "                batch_dict_t, x['file_ID'], x['plate_index'], x['transfer']\n",
    "                ), axis=1)\n",
    "                ) \n",
    "    data['cum_plate'] = data.apply(\n",
    "        lambda x: get_plates(\n",
    "            batch_dict_p, x['file_ID'], x['plate_index']\n",
    "            ), axis=1\n",
    "            )\n",
    "    \n",
    "    # data['strain'] is NA (i.e., negative control) if not yet inoculated\n",
    "    data['strain'] = np.where(\n",
    "        (data['l-t_transfer']>0), \n",
    "        np.array([pd.NA]*len(data)), \n",
    "        data['strain']\n",
    "        )\n",
    "    data['strain'] = data['strain'].astype(\"Int64\")\n",
    "    data['replicate'] = np.where(\n",
    "        pd.isna(data['strain']), \n",
    "        np.array([pd.NA]*len(data)), \n",
    "        data['replicate']\n",
    "        )\n",
    "    data['replicate'] = data['replicate'].astype(\"Int64\")\n",
    "    \n",
    "    # Calculate a background value for each plate reader measurement\n",
    "    # (based on the wells that only contain media)\n",
    "    data['background'] = pd.NA\n",
    "    data['background'] = data.groupby(\n",
    "        ['experiment', 'plate_index', 'timestamp']\n",
    "        )['OD'].transform(\n",
    "        lambda x: x[data.loc[x.index, 'strain'].isna()].mean()\n",
    "        )\n",
    "    \n",
    "    # Compute innoculation timestamp based on the oldest timestamp\n",
    "    data['innoculation_timestamp'] = pd.NA\n",
    "    data.loc[\n",
    "        (~(pd.isna(data['cum_transfer']))) & (~(pd.isna(data['strain']))),\n",
    "        'innoculation_timestamp'\n",
    "        ] = data.groupby(\n",
    "            ['cum_plate', 'cum_transfer']\n",
    "            )['datetime'].transform(\"min\")\n",
    "    \n",
    "    # Assign parent samples\n",
    "    # Only innoculated samples (i.e., not neg. controls) can have parent samples.\n",
    "    # Only samples after passage 1 have parent samples (passage 1 parents will \n",
    "    # have to be manually assigned)\n",
    "    data['parent_plate'] = pd.NA\n",
    "    data.loc[\n",
    "        ~pd.isna(data['strain']) & (data['cum_transfer'] != 1),\n",
    "        'parent_plate'] = data.apply(\n",
    "            lambda x: get_parent_plate(\n",
    "                x['cum_plate'], x['column']\n",
    "                ), axis=1)\n",
    "    data['parent_well'] = pd.NA\n",
    "    data.loc[\n",
    "        ~pd.isna(data['strain']) & (data['cum_transfer'] != 1),\n",
    "        'parent_well'] = data.apply(\n",
    "            lambda x: get_well_name(x['row'], get_parent_col(x['column'])), axis=1)\n",
    "    \n",
    "    # Assign plate, well, and sample names\n",
    "    data = data.convert_dtypes()\n",
    "    data['plate_name'] = (\n",
    "        'E:' + data['experiment'] + '.P:' + data['cum_plate'].astype(str)\n",
    "    )\n",
    "    data['well_name'] = data['plate_name'] + '.W:' + data['well'].astype(str)\n",
    "    data['sample_name']= pd.NA\n",
    "    data.loc[(~(pd.isna(data['gc']))), 'sample_name'] = (\n",
    "        data['well_name'] + '.S:' + data['strain'].astype(str) + '.C:' +\n",
    "        data['gc'].astype(str) + '.R:' + data['replicate'].astype(str) +\n",
    "        '.T:' + data['cum_transfer'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # get parent_id\n",
    "    data['parent_id'] = pd.NA\n",
    "    data.loc[\n",
    "        (~(pd.isna(data['strain']))) &\n",
    "        (~(pd.isna(data['gc']))) &\n",
    "        (data['cum_transfer'] != 1),\n",
    "        'parent_id'] = (\n",
    "            'E:' + data['experiment'] + '.P:' + data['parent_plate'].astype(str) + \n",
    "            '.W:' + data['parent_well'].astype(str) + '.S:' + data['strain'].astype(str) + \n",
    "            '.C:' + data['gc'].astype(str) + '.R:' + data['replicate'].astype(str) + \n",
    "            '.T:' + (data['cum_transfer']-1).astype(str)\n",
    "        )\n",
    "    # data = data.convert_dtypes()\n",
    "    \n",
    "    \n",
    "    # Reformat data for database upload\n",
    "    # 1) Plates\n",
    "    plates = data[\n",
    "        ['plate_name', 'experiment', 'plate_type', 'plate_index', 'layout_filename']\n",
    "        ].drop_duplicates().reset_index(drop=True)\n",
    "    plates.rename(\n",
    "        columns={'plate_name': 'id', 'experiment': 'experiment_id'}, inplace=True\n",
    "        )\n",
    "    plates.to_sql('plate', engine, index=False, if_exists='append')\n",
    "    \n",
    "    # 2) Samples and associated measurements\n",
    "    # Each sample has a measurement of type 'growth' to which all od_measurements map.\n",
    "    sample_meas = data.loc[\n",
    "        ~pd.isna(data['sample_name']) & (~(pd.isna(data['gc'])))\n",
    "        ].drop_duplicates(\n",
    "            (['sample_name', 'experiment', 'plate_name', 'well', 'cum_transfer',\n",
    "              'gc', 'strain', 'innoculation_timestamp', 'replicate', 'parent_id'])\n",
    "                     ).sort_values('innoculation_timestamp')\n",
    "    samples = sample_meas[\n",
    "        (['sample_name', 'experiment', 'plate_name', 'well', 'cum_transfer','gc', \n",
    "          'strain', 'innoculation_timestamp', 'replicate', 'parent_id'])\n",
    "          ].copy()\n",
    "    samples.rename(\n",
    "        columns={\n",
    "            'sample_name': 'name', 'experiment': 'experiment_id', \n",
    "            'plate_name': 'plate', 'cum_transfer': 'passage',\n",
    "            'gc': 'growth_condition_id', 'strain': 'strain_id',\n",
    "            'parent_id': 'parent_sample_id'\n",
    "            }, inplace=True)\n",
    "    samples.to_sql('sample', engine, index=False, if_exists='append')\n",
    "    measurements = sample_meas[\n",
    "        ['sample_name', 'operation_id', 'measurement_type', 'filename']\n",
    "        ].copy()\n",
    "    measurements.rename(\n",
    "        columns={'sample_name': 'sample_id', 'measurement_type': 'type'},\n",
    "        inplace=True\n",
    "        )\n",
    "    measurements.to_sql('measurement', engine, index=False, if_exists='append')\n",
    "    \n",
    "    # 3) OD_measurements\n",
    "    # The index of the db table `measurements` autoincrements, so the measurement\n",
    "    # ids to which the od_measurements of the current dataset belong are not known\n",
    "    # ahead of time. Therefore, need to download all measurements whith sample ids\n",
    "    # from this dataset to get their ids, then merge with OD readings.\n",
    "    sample_names = tuple(sample_meas['sample_name'])\n",
    "    meas_from_db = pd.read_sql(\n",
    "        f\"SELECT `id`, `sample_id` FROM `measurement` WHERE `sample_id` IN {sample_names};\",\n",
    "        engine\n",
    "        ).rename(columns={'id': 'measurement_id'})\n",
    "    od_meas = meas_from_db.merge(\n",
    "        data, left_on='sample_id', right_on='sample_name', how='inner'\n",
    "        )[['measurement_id', 'datetime', 'OD', 'background']].rename(\n",
    "            columns={'OD': 'od'}\n",
    "            )\n",
    "    od_meas.to_sql('od_measurement', engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017e01d-f0f2-4d69-bf1f-9160470b2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330818a1-c397-4930-a8cf-cf3adc57cd6c",
   "metadata": {},
   "source": [
    "## Pull data (and plot)\n",
    "\n",
    "Given an experiment id and a strain id, pull the corresponding samples form the database and present in pandas DataFrame.\n",
    "The function plot_OD() plots the data (using matplotlib).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f97f7-c68e-490c-8930-be92994bdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def query_OD(experiment_id, strain_id):\n",
    "    '''\n",
    "    Queries db for all samples from specified experiment and returns all \n",
    "    associated od_measurements as pandas DataFrame. Plots all samples of\n",
    "    specified strain_id.\n",
    "\n",
    "    Args:\n",
    "        experiment_id (str): Experiment id. Must be in db.\n",
    "        strain_id (str): Strain id. Must be in db.\n",
    "    Returns:\n",
    "        DataFrame    \n",
    "    '''\n",
    "    \n",
    "    # DB Connection\n",
    "    engine = create_engine(\n",
    "        (\n",
    "            \"mysql+pymysql://nspahr:henrylab@poplar.cels.anl.gov/\"\n",
    "            \"anl_synbio?charset=utf8mb4\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Hardcode experiment id. To be changed later\n",
    "    experiment_id = 'ALE1b'\n",
    "\n",
    "    # Check validity of passed arguments\n",
    "    db_experiments = pd.read_sql(\n",
    "        \"SELECT experiment.id FROM experiment\", engine\n",
    "    )['id'].to_list()\n",
    "\n",
    "    db_strains = pd.read_sql(\n",
    "        \"SELECT strain.id FROM strain\", engine\n",
    "    )['id'].to_list()\n",
    "\n",
    "    if (\n",
    "        experiment_id not in db_experiments\n",
    "    ) or (\n",
    "        strain_id not in db_strains\n",
    "    ):\n",
    "        print(f\"Check if experiment and strain are registered in the db.\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    # Hardcoded query. This can be made more flexible later.\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        experiment.id,\n",
    "        sample.name, sample.passage, \n",
    "        sample.strain_id, strain.long_name,\n",
    "        sample.growth_condition_id, growth_condition.carbon_source,\n",
    "        measurement.type,\n",
    "        od_measurement.datetime, od_measurement.od, od_measurement.background\n",
    "    FROM \n",
    "        experiment\n",
    "        INNER JOIN sample ON sample.experiment_id = experiment.id\n",
    "        INNER JOIN measurement ON measurement.sample_id = sample.name\n",
    "        INNER JOIN od_measurement ON od_measurement.measurement_id = measurement.id\n",
    "        INNER JOIN strain ON strain.id = sample.strain_id\n",
    "        INNER JOIN growth_condition ON growth_condition.id = sample.growth_condition_id\n",
    "        \n",
    "    WHERE \n",
    "        (experiment.id=%(experiment)s) AND (sample.strain_id=%(strain)s)\n",
    "    \"\"\"\n",
    "    \n",
    "    selection = pd.read_sql(\n",
    "        query, engine, params={'experiment': experiment_id, 'strain': str(strain_id)}\n",
    "    ).rename(\n",
    "        columns={'id': 'experiment_id',\n",
    "                 'name': 'sample_name',\n",
    "                 'type': 'measurement_type',\n",
    "                 'long_name':'strain_name'}\n",
    "    )\n",
    "\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fc85e-efcd-4649-bbea-a67162802e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = query_OD('not_a_real_argument', 2)\n",
    "selection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2e17b-29ea-4c80-8926-a4326165a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from statistics import mean, median\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_OD(df, subtract_background = False, yscale='log', append_title=''):\n",
    "    '''\n",
    "    Plots OD measurements from DataFrame that is returned from od_query function.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Dataframe returned from od_query() function.\n",
    "        subtract_background (bool): Whether to subtract background reading \n",
    "            from all measurements.\n",
    "        yscale (str): 'log' or 'linear'\n",
    "        append_title (str): Additional text to add to the figure title\n",
    "    Returns:\n",
    "        None    \n",
    "    '''\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    df = df.sort_values('datetime')\n",
    "    df['od_background_subtracted'] = df['od'] - df['background']\n",
    "    \n",
    "    if subtract_background:\n",
    "        value = 'od_background_subtracted'\n",
    "    else:\n",
    "        value = 'od'\n",
    "\n",
    "    # Define different combinations of conditions that will be plotted\n",
    "    conditions = df[['carbon_source', 'strain_name']].drop_duplicates().dropna()\n",
    "    conditions['label'] = conditions.apply(\n",
    "        lambda x: f'{x[\"strain_name\"]} - {x[\"carbon_source\"]}', axis=1\n",
    "    )\n",
    "    conditions['colors'] = colormaps['tab20'].colors[:len(conditions)]\n",
    "\n",
    "    # For the legend\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Figure will need to be stretched horizontally for readability\n",
    "    fig_width, fig_height = fig.get_size_inches() # Get the current figure size\n",
    "    fig.set_size_inches(fig_width * 3.5, fig_height) # Doubling the width\n",
    "    \n",
    "    total_transfers = df['passage'].max()\n",
    "\n",
    "    # For the defined conditions and at each transfer,\n",
    "    # plot the OD readings\n",
    "    for i, row in conditions.iterrows():\n",
    "        handle_line = Line2D([0], [0], label=row['label'], color=row['colors'])\n",
    "        handles.append(handle_line)\n",
    "        label = row['label']\n",
    "        labels.append(label)\n",
    "        \n",
    "        for t in range(total_transfers+1):\n",
    "            this_condition = df.loc[\n",
    "                (df['carbon_source'] == row['carbon_source']) &\n",
    "                (df['strain_name'] == row['strain_name']) &\n",
    "                (df['passage'] == t)\n",
    "            ]\n",
    "            this_condition = this_condition.groupby('datetime'\n",
    "                                                   )[value].agg(mean).to_frame().reset_index()\n",
    "            plt.plot(\n",
    "                this_condition['datetime'],\n",
    "                this_condition[value],\n",
    "                color=row['colors'],\n",
    "                marker='o',\n",
    "                markersize=2\n",
    "            )\n",
    "            \n",
    "    # Configure and label the axes and tickmarks\n",
    "    plt.yscale(yscale)\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "    plt.xlabel('datetime')\n",
    "    plt.ylabel('OD') \n",
    "\n",
    "    early_datetimes = df.groupby('passage')['datetime'].agg(\n",
    "        lambda x: sorted(list(set(x)))[3])\n",
    "    secax = ax.secondary_xaxis('top')\n",
    "    secax.set_xticks(early_datetimes, np.arange(1, total_transfers+1, 1))\n",
    "    secax.set_xlabel('transfer')\n",
    "\n",
    "    # Set title and legend\n",
    "    plt.title(value + append_title, fontsize=16)\n",
    "    plt.legend(handles=handles, labels=labels, loc='lower center')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78e7cc-e5f6-4e2e-982a-924ef6e644da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_OD(selection, subtract_background = False, yscale='log', append_title='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelSEED",
   "language": "python",
   "name": "modelseed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
